{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MarkovProcesses.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOudwN4gm5zmreZOfYz4YPa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjcrofts77/Linear-Systems-MATH30451/blob/main/content/notebooks/Chapter3/MarkovProcesses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maETzr3bf3hN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wewr8jeYf4HT"
      },
      "source": [
        "# 3.3 Markov Processes\n",
        "\n",
        "A *Markov process* describes a system made up of elements that can be in one of several *states*, such that after a transition, the numbers in each state changes according to certain probabilities. Importantly, these probabilities only depend on the current state, *i.e.* a Markov process is *memoryless*. (The simplest example is a simple coin toss -- here there are 2 states, heads or tails, and the probability of being in either state given the current state is 0.5 regardless, for an unbiased coin at least!)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.5 (Nottingham Weather)** Model assumptions:\n",
        "\n",
        " - The best way to predict tomorrow's weather is to say it will be the same as today.\n",
        "\n",
        " - Two types of weather: sunshine or rain.\n",
        "\n",
        "Supposing the above predictor is correct 75% of the time, we can model the weather as a two state Markov process.\n",
        "```{margin} State Diagram\n",
        "The figure to the left shows the *state diagram* for the Markov process of our simple Nottingham weather model. This is a graph with *nodes* denoting the states of the system and directed *edges* the probabilities of moving between said states.\n",
        "```\n",
        "```{figure} ../../images/NottsWeather.png\n",
        "---\n",
        "height: 250px\n",
        "---\n",
        "```\n",
        "\n",
        "In matrix form\n",
        "\n",
        "$$\n",
        "P = \\begin{pmatrix} \\frac{3}{4}&\\frac{1}{4}\\\\\\frac{1}{4}&\\frac{3}{4}\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "```{note}\n",
        "1. We say that $P$ is a *stochastic matrix* since\n",
        "<br><br>\n",
        " $\n",
        "\\sum_iP_{ij} = \\sum_jP_{ij} = 1. \\qquad\\color{red}{\\text{ (since the entries are probabilities)}}\n",
        " $\n",
        "<br><br>\n",
        "2. Typically $P$ will only be *row stochastic* meaning that $\\displaystyle\\sum_jP_{ij}=1$.<br><br>\n",
        "3. The above formulation provides a simple model for describing the Nottingham weather:\n",
        "\n",
        "$$\n",
        "  \\mathbf{x}_{k+1} &= P^T\\mathbf{x}_k, \\qquad \\mathbf{x}_0 = \\mathbf{c}\\\\\n",
        "  &= \\begin{pmatrix} \\frac{3}{4}&\\frac{1}{4}\\\\\\frac{1}{4}&\\frac{3}{4}\\end{pmatrix}\\mathbf{x}_k, \\qquad \\mathbf{x}_0 = \\mathbf{c}. \\qquad \\color{red}{(\\mathbf{x}_k - \\text{prob of rain or sunshine on } k^{\\mathrm{th}} \\text{ day})}\n",
        "$$\n",
        "\n",
        "Multiplying out, we see that\n",
        "\n",
        "$$\n",
        "  x_{k+1}^{(1)} &=\\frac{3}{4}x_k^{(1)}+\\frac{1}{4}x_k^{(2)},\\qquad x_0^{(1)}=c^{(1)}\\\\\n",
        "    x_{k+1}^{(2)}&=\\frac{1}{4}x_k^{(1)}+\\frac{3}{4}x_k^{(2)},\\qquad x_0^{(2)}=c^{(2)}.\n",
        "\n",
        "$$\n",
        "Now, suppose $\\displaystyle\\mathbf{c}^T = \\begin{pmatrix} 1&0\\end{pmatrix}$, *i.e.* it is sunny today, then\n",
        "\n",
        "$$\n",
        " \\begin{pmatrix} x^{(1)}_1\\\\x^{(2)}_1\\end{pmatrix} &=\\begin{pmatrix}\\frac{3}{4}\\\\\\frac{1}{4}\\end{pmatrix} \\qquad\\color{red}{\\text{ 75% chance of sunshine; 25% chance of rain}}\\\\\n",
        "  \\begin{pmatrix} x^{(1)}_2\\\\x^{(2)}_2\\end{pmatrix} &=\\begin{pmatrix}\\frac{3}{4}\\cdot\\frac{3}{4}+\\frac{1}{4}\\cdot\\frac{1}{4}\\\\\\frac{1}{4}\\cdot\\frac{3}{4}+\\frac{3}{4}\\cdot\\frac{1}{4}\\end{pmatrix}\\\\\n",
        "  &= \\begin{pmatrix} \\frac{10}{16}\\\\\\frac{6}{16}\\end{pmatrix} \\qquad\\color{red}{\\text{ 62% chance of sunshine; 38% chance of rain}}\n",
        "$$\n",
        "\n",
        "Importantly, in the above model we see that uncertainty inceases as we move further into the future, as expected.\n",
        "```\n",
        "\n",
        "In the above example, the characteristic polynomial is given by\n",
        "\n",
        "$$\n",
        "\\chi_{P^T}(t) = \\left(t-1\\right)\\left(t-\\frac{1}{2}\\right)\n",
        "$$\n",
        "\n",
        "thus $\\lambda =1$ is the dominant eigenvalue and so for large $k$, $\\displaystyle \\mathbf{x}_k$ will tend to a constant vector $\\mathbf{x}^*$. It can be shown that $\\displaystyle \\mathbf{x}^*$ is given by the eigevector, $\\mathbf{v}$, of $\\displaystyle P^T$ corresponding to the eigenvalue $\\lambda = 1$.\n",
        "\n",
        "\n",
        "Thus\n",
        "\n",
        "$$\n",
        " \\mathbf{x}^* = \\begin{pmatrix} 0.5\\\\0.5\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "in our case. (*i.e.* long-term weather forecasting is doomed -- at least with our simple model.) \n",
        "\n",
        "\n",
        "Actually\n",
        "\n",
        "$$\n",
        " \\mathbf{x}^* = \\frac{\\mathbf{v}}{\\sum_i v^{(i)}},\n",
        "$$\n",
        "\n",
        "as $\\displaystyle \\mathbf{x}^*$ is a probability vector.\n",
        "\n",
        "<br>\n",
        "\n",
        "More generally, we can define an $n$-state Markov process by a *transition matrix* $\\displaystyle P\\in\\mathbb{R}^{n\\times n}$ given by\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        " P = (p_{ij}) = \\begin{pmatrix}\n",
        "                 p_{11}&p_{12}&\\ldots&p_{1n}\\\\\n",
        "                 p_{21}&p_{22}&\\ldots&p_{2n}\\\\\n",
        "                 \\vdots&\\vdots&&\\vdots\\\\\n",
        "                 p_{n1}&p_{n2}&\\ldots&p_{nn} \n",
        "                \\end{pmatrix},\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "where $\\displaystyle 0\\leq p_{ij} \\leq 1$ $(i,j=1,2,\\ldots, n)$ and $\\sum_{j=1}^np_{ij}=1$.\n",
        "\n",
        "It is clear that the connection between the state vector \n",
        "$\\displaystyle\\mathbf{x}_k=[x^{(1)}_k,x^{(2)}_k,\\ldots,x^{(n)}_k]^T$ at one particular stage\n",
        "and $\\displaystyle \\mathbf{x}_{k+1}=[x^{(1)}_{k+1},x^{(2)}_{k+1},\\ldots,x^{(n)}_{k+1}]^T$ at \n",
        "the next is given by\n",
        "\n",
        "$$\n",
        " \\mathbf{x}_{k+1} = P^T\\mathbf{x}_k.\n",
        "$$\n",
        "\n",
        "Then, given an initial state vector $\\displaystyle\\mathbf{x}_0$, we know that \n",
        "$\\displaystyle\\mathbf{x}_k=\\left(P^T\\right)^k\\mathbf{x}_0$.\n",
        "\n",
        "Very often we do not construct $\\displaystyle\\mathbf{x}_k$ from the numbers in each of the \n",
        "states but from the proportions of the total number of elements in the process\n",
        "which are in each of these states. Then, clearly $\\displaystyle 0\\leq x^{(i)}_k\\leq 1$ and \n",
        "$\\displaystyle\\sum_{i=1}^nx^{(i)}_k=1$. In this case, we can call $\\displaystyle\\mathbf{x}_k$ a \n",
        "probability vector.\n",
        "\n",
        "As we have seen before, the long term behaviour of a Markov process depends on \n",
        "the eigenvalues of the matrix $P^T$ and it is easy to show that one of these \n",
        "eigenvalues is $1$. For, consider\n",
        "\n",
        "$$\n",
        " |P^T-I_n|&=\\left|\\begin{matrix}\n",
        "                   p_{11}-1&p_{21}&\\ldots&p_{n1}\\\\\n",
        "                   p_{12}&p_{22}-1&\\ldots&p_{n2}\\\\\n",
        "                   \\vdots&\\vdots&&\\vdots\\\\\n",
        "                   p_{1n}&p_{2n}&\\ldots&p_{nn}-1\n",
        "                  \\end{matrix}\n",
        "\\right|,\\\\\n",
        " &=\\left|\\begin{matrix}\n",
        "                   \\sum_{i=1}^np_{1i}-1&\\sum_{i=1}^np_{2i}-1&\n",
        "                   \\ldots&\\sum_{i=1}^np_{ni}-1\\\\\n",
        "                   p_{12}&p_{22}-1&\\ldots&p_{n2}\\\\\n",
        "                   \\vdots&\\vdots&&\\vdots\\\\\n",
        "                   p_{1n}&p_{2n}&\\ldots&p_{nn}-1\n",
        "                  \\end{matrix}\n",
        "\\right|,\\\\  &=\\left|\\begin{matrix}\n",
        "                   0&0&\\ldots&0\\\\\n",
        "                   p_{12}&p_{22}-1&\\ldots&p_{n2}\\\\\n",
        "                   \\vdots&\\vdots&&\\vdots\\\\\n",
        "                   p_{1n}&p_{2n}&\\ldots&p_{nn}-1\n",
        "                  \\end{matrix}\n",
        "\\right|=0.\n",
        "$$\n",
        "\n",
        "We have thus shown that there is a potential stationary vector for this process,\n",
        "this being the eigenvector (of $\\displaystyle P^T$ or equivalently the left-eigenvector of\n",
        "$\\displaystyle P$) which corresponds to the eigenvalue $1$.\n",
        "\n",
        "There are then two questions which we can ask about this eigenvector and the\n",
        "first is whether we can be sure that its elements are all non-negative, so \n",
        "that it can conform to the requirement of being a probability vector. The\n",
        "second question concerns whether the Markov process under consideration will \n",
        "always settle down to this vector eventually, whatever the starting point \n",
        "$\\displaystyle\\mathbf{x}_0$.\n",
        "\n",
        "We can answer both of these questions in the affirmative if it can be shown that\n",
        "the eigenvalue $1$ is strictly dominant, as was the case for the eigenvalue\n",
        "$\\displaystyle\\lambda_1$ of a Leslie matrix under certain conditions.\n",
        "\n",
        "Now, the dominance of the eigenvalue $1$ can be established by means of a result\n",
        "that states that the magnitude of any eigenvalue of a given matrix does not\n",
        "exceed $\\displaystyle\\min(\\rho,\\gamma)$, where $\\rho$ is the maximum sum of the magnitudes of\n",
        "the elements in any one of its rows and $\\gamma$ is the corresponding maximum\n",
        "for its columns.\n",
        "\n",
        "Clearly, for a transition matrix $\\displaystyle\\min(\\rho,\\gamma)\\leq 1$, so the eigenvalue\n",
        "$1$ is dominant. However, as we can see from the following example, it is not \n",
        "necessarily strictly dominant.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Example 3.2.6** The matrix \n",
        "\n",
        "$$\n",
        "P_1=\\begin{pmatrix}0&1\\\\1&0\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "has eigenvalues $\\pm{1}$. Also, the probability eigenvector corresponding \n",
        "to the eigenvalue $1$ is $[\\frac{1}{2}, \\frac{1}{2}]^T$ and it is easy to \n",
        "see that there are starting points which do not lead to this vector. For example, if\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_0 = \\begin{pmatrix}\n",
        "                                        0\\\\1\n",
        "                                       \\end{pmatrix},\n",
        "\\mathbf{x}_1=\\begin{pmatrix}\n",
        " 1\\\\0\n",
        "\\end{pmatrix}=\\mathbf{x}_3=\\mathbf{x}_5,\\ldots, \\quad\n",
        "\\mathbf{x}_2=\\begin{pmatrix}\n",
        "              1\\\\0\n",
        "             \\end{pmatrix}=\\mathbf{x}_4=\\mathbf{x}_6,\\ldots.        \n",
        "$$\n",
        "\n",
        "There is a condition which establishes the strict dominance of the eigenvalue,\n",
        "and this is that the transition matrix $P$ should be regular, in that there is\n",
        "some power $P^r$ which has elements which are all positive. When this occurs \n",
        "we know that the Markov process under consideration will always settle down\n",
        "to the appropriate stationary vector.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.7** The matrix $P_1$ above is not regular because it is easy \n",
        "to see that \n",
        "\n",
        "$$\n",
        " P^{2r}=I_2\\qquad\\&\\qquad P^{2r+1}=P_1\\qquad (r=0,1,2,\\ldots).\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.8** The matrix\n",
        "\n",
        "$$\n",
        "P_2=\\begin{pmatrix}0.2&0.8\\\\0.6&0.4\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "is clearly regular and has eigenvalues given by\n",
        "\n",
        "$$\n",
        " |P-\\lambda I_2| = (\\lambda-1)(\\lambda+0.4)=0 \\qquad \n",
        " \\text{i.e., $\\lambda=1$ or $\\lambda=-0.4$.}\n",
        "$$\n",
        "\n",
        "The left eigenvector corresponding to\n",
        "$\\lambda=1$ is given by \n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}-0.8&0.6\\\\0.8&-0.6\\end{pmatrix}\\begin{pmatrix} u_1\\\\u_2\\end{pmatrix}=\\mathbf{0},\n",
        "$$\n",
        "\n",
        "and is therefore of the form $\\displaystyle [3a, 4a]^T$.\n",
        "\n",
        "As a probability vector, we have \n",
        "\n",
        "$$\n",
        "\\mathbf{u}=\\begin{pmatrix}\\frac{3}{7}\\\\\\frac{4}{7}\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "For example, suppose \n",
        "\n",
        "$$\n",
        "\\mathbf{x}_0=\\begin{pmatrix}0.5\\\\0.5\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Then\n",
        "\n",
        "$$\n",
        " \\mathbf{x}_1=\\begin{pmatrix} 0.4\\\\0.6\\end{pmatrix}, \\mathbf{x}_2=\\begin{pmatrix} 0.44\\\\0.56\\end{pmatrix}, \n",
        " \\mathbf{x}_3=\\begin{pmatrix} 0.424\\\\0.576\\end{pmatrix} \\text{ and } \n",
        " \\mathbf{x}_4=\\begin{pmatrix} 0.4304\\\\0.5696\\end{pmatrix} \\qquad \n",
        " \\left(\\text{c.f.} \\begin{pmatrix}\\frac{3}{7}\\\\\\frac{4}{7}\\end{pmatrix}=\n",
        " \\begin{pmatrix} 0.4286\\\\0.5714\\end{pmatrix}\\right).\n",
        "$$"
      ]
    }
  ]
}
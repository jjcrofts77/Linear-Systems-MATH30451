{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HomoDiff.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjcrofts77/Linear-Systems-MATH30451/blob/main/content/notebooks/Chapter3/HomoDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPPuTtrqJEUe"
      },
      "source": [
        "# 3.2 Homogeneous Systems of Difference Equations\n",
        "\n",
        "Consider a situation in which we have a $n\\times n$ matrix $A$ and a sequence of vectors $\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_k,\\ldots$ which evolve in a way that can be described by a system of difference equations of the form: \n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{k+1} = A\\mathbf{x}_k, \\qquad \\mathbf{x}_0 = \\mathbf{c}\\in\\mathbb{R}^n.\n",
        "$$\n",
        "\n",
        "Such a system can be solved by diagonalising the matrix $A$, if this is possible, in the same manner as used previously for a system of differential equations of the form $\\dot{\\mathbf{x}} = A\\mathbf{x}$.  \n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.1** Suppose $\\displaystyle A_1=HDH^{-1}$, where\n",
        "\n",
        "$$\n",
        " A_1 = \\begin{pmatrix}\\frac{1}{2}&\\frac{1}{2}&\\frac{1}{2}\\\\                     \n",
        "    -\\frac{1}{6}&\\frac{1}{2}&\\frac{1}{6}\\\\\n",
        "     \\frac{1}{6}&\\frac{1}{2}&\\frac{5}{6}\n",
        "     \\end{pmatrix}, \\quad\n",
        " H = \\begin{pmatrix}1&0&1\\\\-1&1&0\\\\1&-1&1\\end{pmatrix},\\quad\n",
        " H^{-1}=\\begin{pmatrix}1&-1&-1\\\\1&0&-1\\\\0&1&1\\end{pmatrix},\\quad\n",
        " D=\\begin{pmatrix}\\frac{1}{2}&0&0\\\\0&\\frac{1}{3}&0\\\\0&0&1\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "and we have the system $\\displaystyle \\mathbf{x}_{k+1}=A_1\\mathbf{x}_k:$ $\\mathbf{x}_0=[0, 0, 1]^T$.\n",
        "\n",
        "Now $\\displaystyle\\mathbf{x}_{k+1}=HDH^{-1}\\mathbf{x}_k$ or $\\displaystyle H^{-1}\\mathbf{x}_{k+1} = DH^{-1}\\mathbf{x}_k$, therefore taking $\\displaystyle\\mathbf{y}_{k}=H^{-1}\\mathbf{x}_k$ gives a system of the form\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\mathbf{y}_{k+1} = D\\mathbf{y}_k \\quad :\\quad \n",
        "\\mathbf{y}_0 = H^{-1}\\mathbf{x}_0 = \\begin{pmatrix}-1\\\\-1\\\\1\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "We then have \n",
        "\n",
        "$$\n",
        " \\begin{pmatrix}\n",
        " y^{(1)}_{k+1}\\\\y^{(2)}_{k+1}\\\\y^{(3)}_{k+1}\n",
        " \\end{pmatrix} =\n",
        " \\begin{pmatrix}\n",
        " \\frac{1}{2}&0&0\\\\0&\\frac{1}{3}&0\\\\0&0&1\n",
        " \\end{pmatrix}\n",
        " \\begin{pmatrix}\n",
        " y^{(1)}_k\\\\y^{(2)}_k\\\\y^{(3)}_k\n",
        " \\end{pmatrix} \\quad\n",
        " \\text{and} \\quad\n",
        " \\begin{pmatrix}\n",
        " y^{(1)}_0\\\\y^{(2)}_0\\\\y^{(3)}_0\n",
        " \\end{pmatrix} = \n",
        " \\begin{pmatrix}\n",
        " -1\\\\-1\\\\1\\end{pmatrix}, \n",
        "$$\n",
        "\n",
        "*i.e.*,\n",
        "\n",
        "$$\n",
        " y^{(1)}_{k+1}=\\frac{1}{2}y^{(1)}_k \\quad &:\\quad y^{(1)}_0 = -1 \\quad \n",
        " \\implies \\quad y^{(1)}_k=-\\left(\\frac{1}{2}\\right)^k,\\\\\n",
        " y^{(2)}_{k+1}=\\frac{1}{3}y^{(2)}_k \\quad &: \\quad y^{(2)}_0=-1 \\quad \n",
        " \\implies \\quad y^{(2)}_k=-\\left(\\frac{1}{3}\\right)^k,\\\\\n",
        " y^{(3)}_{k+1} = y^{(3)}_k \\quad &: \\quad y^{(3)}_k \\quad \\implies \\quad\n",
        " y^{(3)}_k = 1,\n",
        "$$\n",
        "\n",
        "therefore $\\displaystyle \\mathbf{y}_k = [-\\left(\\frac{1}{2}\\right)^k, \n",
        "-\\left(\\frac{1}{3}\\right)^k, 1]^T$ and\n",
        "\n",
        "$$\n",
        " \\mathbf{x}_k = H\\mathbf{y}_k = \\begin{pmatrix} 1-\\left(\\frac{1}{2}\\right)^k\\\\\n",
        " \\left(\\frac{1}{2}\\right)^k-\\left(\\frac{1}{3}\\right)^k\\\\\n",
        " 1-\\left(\\frac{1}{2}\\right)^k+\\left(\\frac{1}{3}\\right)^k\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Of course, it is not always possible to diagonalise a matrix $A$ by a \n",
        "transformation of similarity. However, it is always possible to bring a square\n",
        "matrix to a Jordan form by such a transformation and it is then relatively\n",
        "simple to modify the above procedure to solve the system under consideration.\n",
        "\n",
        "Although we have illustrated the above method for solving a system of the form\n",
        "$\\displaystyle \\mathbf{x}_{k+1}=A\\mathbf{x}_k$, it would seem much easier to determine its \n",
        "solution by observing that, given $\\mathbf{x}_0$, it is clear that\n",
        "\n",
        "$$\n",
        " \\mathbf{x}_{k} = A^k\\mathbf{x}_0.\n",
        "$$\n",
        "\n",
        "Then, if we diagonalise $A$ by a transformation of similarity, we have\n",
        "\n",
        "$$\n",
        " \\mathbf{x}_k = \\left(HDH^{-1}\\right)^k\\mathbf{x}_0 = \n",
        "(HDH^{-1})(HDH^{-1})\\cdots(HDH^{-1})\\mathbf{x}_0 = HD^kH^{-1}\\mathbf{x}_0,\n",
        "$$\n",
        "\n",
        "where $D^k$ is of the form \n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\\lambda_1^k&&&\\\\&\\lambda_2^k&&0\\\\\n",
        "0&&\\ddots&\\\\&&&\\lambda_n^k\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.2** For the system considered in example 3.1,\n",
        "\n",
        "$$\n",
        " \\mathbf{x}_k = HD^kH^{-1}\\mathbf{x}_0 &= \\begin{pmatrix} 1&0&1\\\\-1&1&0\\\\1&-1&1\\end{pmatrix}\n",
        " \\begin{pmatrix} \\left(\\frac{1}{2}\\right)^k&0&0\\\\0&\\left(\\frac{1}{3}\\right)^k\n",
        " &0\\\\0&0&1\\end{pmatrix}\\begin{pmatrix} 1&-1&-1\\\\1&0&-1\\\\0&1&1\\end{pmatrix}\\begin{pmatrix} 0\\\\0\\\\1\\end{pmatrix},\\\\\n",
        " &=\\begin{pmatrix} 1&0&1\\\\-1&1&0\\\\1&-1&1\\end{pmatrix}\n",
        " \\begin{pmatrix} \\left(\\frac{1}{2}\\right)^k&0&0\\\\0&\\left(\\frac{1}{3}\\right)^k\n",
        " &0\\\\0&0&1\\end{pmatrix}\\begin{pmatrix}-1\\\\-1\\\\1\\end{pmatrix},\\\\&=\\begin{pmatrix} 1&0&1\\\\-1&1&0\\\\1&-1&1\\end{pmatrix}\n",
        " \\begin{pmatrix}-\\left(\\frac{1}{2}\\right)^k\\\\-\\left(\\frac{1}{3}\\right)^k\\\\1\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "as before.\n",
        "\n",
        "The situation is a little more complicated if a transformation of similarity \n",
        "only brings $A$ to Jordan form.\n",
        "\n",
        "$$\n",
        " \\text{i.e., if}\\quad A=HJH^{-1}, \\quad \\text{where}\\quad \n",
        " J=\\begin{pmatrix} J_{n_1}(\\lambda_1)&0&\\ldots&0\\\\0&J_{n_2}(\\lambda_2)&\\ldots&0\\\\\\vdots&&\\ddots&\\vdots\\\\\n",
        " 0&0&\\ldots&J_{n_k}(\\lambda_k)\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "and each $\\displaystyle J_{n_i}(\\lambda_i)$ is either a diagonal matrix or a Jordan block of the form\n",
        "\n",
        "$$\n",
        " J_{n_i}(\\lambda_i)=\\begin{pmatrix}\\lambda_i&1&0&\\ldots&0\\\\0&\\lambda_i&1&\\ldots&0\\\\\n",
        " \\vdots&\\vdots&\\vdots&&\\vdots\\\\0&0&0&\\ldots&\\lambda_i\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Then \n",
        "\n",
        "$$\n",
        "J^r=\\begin{pmatrix} J_{n_1}^r&0&\\ldots&0\\\\0&J_{n_2}^r&\\ldots&\\vdots\\\\\n",
        "\\vdots&\\vdots&\\ddots&\\vdots\\\\0&0&\\ldots&J_{n_k}^r\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "where $\\displaystyle J_i^r$ is either a diagonal matrix or of the form illustrated earlier in the discussion on powers of Jordan blocks.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.3**\n",
        "\n",
        "$$\n",
        "\\text{If}~ A_2=\\begin{pmatrix}\n",
        "3&0&0&0&0&0\\\\0&2&0&0&0&0\\\\0&0&2&0&0&0\\\\0&0&0&-1&1&0\\\\\n",
        "0&0&0&0&-1&1\\\\0&0&0&0&0&-1\n",
        "\\end{pmatrix}=\n",
        "\\begin{pmatrix}\n",
        "J_1(3)&0&0\\\\0&J_2(2)&0\\\\0&0&J_3(-1)\n",
        "\\end{pmatrix},~\\text{where}\n",
        "$$\n",
        "\n",
        "$J_1(3)=3$, $\\displaystyle J_2(2)=\\begin{pmatrix}2&0\\\\0&2\\end{pmatrix}$ and\n",
        "$\\displaystyle J_3(-1)=\\begin{pmatrix}-1&1&0\\\\0&-1&1\\\\0&0&-1\\end{pmatrix}$ is a $3\\times 3$ Jordan block for which\n",
        "\n",
        "$$\n",
        "J^r_3=\\begin{pmatrix}(-1)^r&\\binom{r}{1}(-1)^{r-1}&\\binom{r}{2}(-1)^{r-2}\\\\\n",
        "0&(-1)^r&\\binom{r}{1}(-1)^{r-1}\\\\0&0&(-1)^r\\end{pmatrix} = \n",
        "\\begin{pmatrix}(-1)^r&r(-1)^{r-1}&\\frac{r(r-1)}{2}(-1)^{r-2}\\\\\n",
        "0&(-1)^r&r(-1)^{r-1}\\\\0&0&(-1)^r\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "then\n",
        "\n",
        "$$\n",
        "A_2^r=\\begin{pmatrix}\n",
        "J_1^r&0&0\\\\0&J_2^r&0\\\\0&0&J_3^r\n",
        "\\end{pmatrix}=\\begin{pmatrix}\n",
        "3^r&0&0&0&0&0\\\\0&2^r&0&0&0&0\\\\0&0&2^r&0&0&0\\\\0&0&0&(-1)^r&r(-1)^{r-1}&\\frac{\n",
        "r(r-1)}{2}(-1)^{r-2}\\\\\n",
        "0&0&0&0&(-1)^r&r(-1)^{r-1}\\\\0&0&0&0&0&(-1)^r\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        " \n",
        "Given $\\mathbf{x}_0$, the solution of a system of the form $\\displaystyle\\mathbf{x}_{k+1}=A\\mathbf{x}_k$ for a matrix that can be brought to Jordan form\n",
        "by a transformation of similarity, as above, can then be obtained by simple, but\n",
        "probably tedious, matrix multiplication.\n",
        "\n",
        "## Long-term behaviour\n",
        "\n",
        "Now, quite often we are not interested in the full solution of a system of the\n",
        "above form but only in what happens in the long term, *i.e.*, as $k\\to\\infty$. If we remember that \n",
        "\n",
        "$$\n",
        "\\mathbf{x}_k\n",
        "=HJ^kH^{-1}\\mathbf{x}_0=H\n",
        "\\begin{pmatrix} J^k_{n_1}&0&\\ldots&0\\\\\n",
        "0&J^k_{n_2}&\\ldots&0\\\\\\vdots&\\vdots&&\\vdots\\\\\n",
        "0&0&\\ldots&J^k_{n_q}\\end{pmatrix} H^{-1}\\mathbf{x}_0,\n",
        "$$\n",
        "\n",
        "we see that:\n",
        "\n",
        "1. if $|\\lambda_i|<1$ for all eigenvalues of $A$, then $\\mathbf{x}_k\\to\n",
        "0$ as $k\\to\\infty$;\n",
        "\n",
        "2. if $|\\lambda_i|>1$ for any of these eigenvalues, then\n",
        "$\\mathbf{x}_k$ is unbounded for general $\\mathbf{x}_0$;\n",
        "\n",
        "3. if $|\\lambda_i|\\leq 1$ for all eigenvalues of $A$, then the\n",
        "situation is unclear unless we know that the only eigenvalue of modulus one\n",
        "*is* one and that this eigenvalue is unrepeated, i.e., that the\n",
        "eigenvalue $1$ is *strictly dominant*.\n",
        "\n",
        "Suppose that in the last case we are able to arrange things so that the\n",
        "eigenvalue $1$ occurs in the last row and column of $J$.\n",
        "\n",
        "$$\n",
        "\\text{We then have }J^r=\\begin{pmatrix} J^r_{n_1}&0&\\ldots&0\\\\0&J^r_{n_2}&\\ldots&0\\\\\n",
        "\\vdots&\\vdots&&\\vdots\\\\0&0&\\ldots&J^r_{n_q}\\end{pmatrix} \\to\n",
        "\\begin{pmatrix} 0&0&\\ldots&0\\\\0&0&\\ldots&0\\\\\\vdots&\\vdots&&\\vdots\\\\0&0&\\ldots&1\\end{pmatrix}\n",
        "\\text{ as } r\\to\\infty.\n",
        "$$\n",
        "\n",
        "Therefore \n",
        "\n",
        "$$\n",
        "\\mathbf{x}_k=HJ^kH^{-1}\\mathbf{x}_0\\to H\\begin{pmatrix}0&0\\\\0&1\\end{pmatrix}\n",
        "H^{-1}\\mathbf{x}_0=H\\begin{pmatrix}0\\\\\\mathbf{\\hat{h}}_n\\end{pmatrix}\\mathbf{x}_0,\n",
        "$$\n",
        "\n",
        "where $\\displaystyle \\mathbf{\\hat{h}}_n$ is the last row of $H^{-1}$. Also, $\\displaystyle\\mathbf{x}_k\\to \\mathbf{h}_n\\mathbf{\\hat{h}}_n\\mathbf{x}_0$, where $\\mathbf{h}_n$ is the last column of $H$. In other words $\\mathbf{x}_k$ tends towards a fixed vector in the long run.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.4** For $A_1$ in example 3.1, $\\displaystyle\\mathbf{h}_3=[1, 0, 1]^T, \\mathbf{\\hat{h}}_3=[0,1,1]$ and $\\displaystyle\\mathbf{x}_0=[0,0,1]^T$. \n",
        "\n",
        "Therefore\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_k\\to\\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix}\\begin{pmatrix}0&1&1\\end{pmatrix}\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix}\n",
        "1=\\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "as can be seen from the solution found previously.\n",
        "\n",
        "The result above is the special case of the situation in which the matrix $A$ has *any* strictly dominant eigenvalue $\\lambda$. In this case the process will usually settle down from a general starting point to give\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{k+1}=\\lambda\\mathbf{x}_k,\n",
        "$$\n",
        "\n",
        "in the long term. This of course is the basis of the *power method* for finding\n",
        "any strictly dominant eigenvalue of a matrix.\n",
        "\n",
        "Having discussed systems of the form $\\displaystyle\\mathbf{x}_{k+1}=A\\mathbf{x}_k$ in general, we shall now turn our attention to two particular situations that can be described in this way.\n",
        "\n",
        "## Markov Processes\n",
        "\n",
        "A *Markov process* describes a system made up of elements that can be in one of several *states*, such that after a transition, the numbers in each state changes according to certain probabilities. Importantly, these probabilities only depend on the current state, *i.e.* a Markov process is *memoryless*. (The simplest example is a simple coin toss -- here there are 2 states, heads or tails, and the probability of being in either state given the current state is 0.5 regardless, for an unbiased coin at least!)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.5 (Nottingham Weather)** Model assumptions:\n",
        "\n",
        " - The best way to predict tomorrow's weather is to say it will be the same as today.\n",
        "\n",
        " - Two types of weather: sunshine or rain.\n",
        "\n",
        "Supposing the above predictor is correct 75% of the time, we can model the weather as a two state Markov process.\n",
        "\n",
        "```{figure}../../images/NottsWeather.png\n",
        "```\n",
        "\n",
        "In matrix form\n",
        "\n",
        "$$\n",
        "P = \\begin{pmatrix} \\frac{3}{4}&\\frac{1}{4}\\\\\\frac{1}{4}&\\frac{3}{4}\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "```{note}\n",
        "1. We say that $P$ is a *stochastic matrix* since\n",
        "\n",
        " $$\n",
        " \\sum_iP_{ij} = \\sum_jP_{ij} = 1. \\qquad\\textcolor{red}{\\text{ (since the entires are probabilities)}}\n",
        "$$\n",
        "\n",
        "2. Typically $P$ will only be *row stochastic* meaning that $\\displaystyle\\sum_jP_{ij}=1$.\n",
        "\n",
        "3. The above formulation provides a simple model for describing the Nottingham weather:\n",
        "\n",
        " $$\n",
        "  \\mathbf{x}_{k+1} &= P^T\\mathbf{x}_k, \\qquad \\mathbf{x}_0 = \\mathbf{c}\\\\\n",
        "  &= \\bmc \\frac{3}{4}&\\frac{1}{4}\\\\\\frac{1}{4}&\\frac{3}{4}\\emc\\mathbf{x}_k, \\qquad \\mathbf{x}_0 = \\mathbf{c}. \\qquad \\textcolor{red}{(\\mathbf{x}_k - \\text{prob of rain or sunshine on } k^{\\mathrm{th}} \\text{ day})}\n",
        "$$\n",
        "\n",
        "Multiplying out, we see that\n",
        "\n",
        "$$\n",
        "  x_{k+1}^{(1)} &=\\frac{3}{4}x_k^{(1)}+\\frac{1}{4}x_k^{(2)},\\qquad x_0^{(1)}=c^{(1)}\\\\\n",
        "    x_{k+1}^{(2)}&=\\frac{1}{4}x_k^{(1)}+\\frac{3}{4}x_k^{(2)},\\qquad x_0^{(2)}=c^{(2)}.\n",
        "$$\n",
        "\n",
        "Now, suppose $\\displaystyle\\mathbf{c}^T = \\bmc 1&0\\emc$, *i.e.* it is sunny today, then\n",
        "\n",
        " $$\n",
        " \\egin{pmatrix} x^{(1)}_1\\\\x^{(2)}_1\\emc &=\\bmc\\frac{3}{4}\\\\\\frac{1}{4}\\end{pmatrix} \\qquad\\textcolor{red}{\\text{ 75\\% chance of sunshine; 25\\% chance of rain}}\\\\\n",
        "  \\begin{pmatrix} x^{(1)}_2\\\\x^{(2)}_2\\emc &=\\bmc\\frac{3}{4}\\cdot\\frac{3}{4}+\\frac{1}{4}\\cdot\\frac{1}{4}\\\\\\frac{1}{4}\\cdot\\frac{3}{4}+\\frac{3}{4}\\cdot\\frac{1}{4}\\end{pmatrix}\\\\\n",
        "  &= \\begin{pmatrix} \\frac{10}{16}\\\\\\frac{6}{16}\\end{pmatrix} \\qquad\\textcolor{red}{\\text{ 62\\% chance of sunshine; 38\\% chance of rain}}\n",
        " $$\n",
        "\n",
        "Importantly, in the above model we see that uncertainty inceases as we move further into the future, as expected.\n",
        "```\n",
        "\n",
        "In the above example, the characteristic polynomial is given by\n",
        "\n",
        "$$\n",
        "\\chi_{P^T}(t) = \\left(t-1\\right)\\left(t-\\frac{1}{2}\\right)\n",
        "$$\n",
        "\n",
        "thus $\\lambda =1$ is the dominant eigenvalue and so for large $k$, $\\displaystyle \\mathbf{x}_k$ will tend to a constant vector $\\mathbf{x}^*$. It can be shown that $\\displaystyle \\mathbf{x}^*$ is given by the eigevector, $\\mathbf{v}$, of $\\displaystyle P^T$ corresponding to the eigenvalue $\\lambda = 1$.\n",
        "\n",
        "\n",
        "Thus\n",
        "\n",
        "$$\n",
        " \\mathbf{x}^* = \\bmc 0.5\\\\0.5\\emc\n",
        "$$\n",
        "\n",
        "in our case. (*i.e.* long-term weather forecasting is doomed -- at least with our simple model.) \n",
        "\n",
        "\n",
        "Actually\n",
        "\n",
        "$$\n",
        " \\mathbf{x}^* = \\frac{\\mathbf{v}}{\\sum_i v^{(i)}},\n",
        "$$\n",
        "\n",
        "as $\\displaystyle \\mathbf{x}^*$ is a probability vector.\n",
        "\n",
        "<br>\n",
        "\n",
        "More generally, we can define an $n$-state Markov process by a *transition matrix* $\\displaystyle P\\in\\mathbb{R}^{n\\times n}$ given by\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        " P = (p_{ij}) = \\begin{pmatrix}\n",
        "                 p_{11}&p_{12}&\\ldots&p_{1n}\\\\\n",
        "                 p_{21}&p_{22}&\\ldots&p_{2n}\\\\\n",
        "                 \\vdots&\\vdots&&\\vdots\\\\\n",
        "                 p_{n1}&p_{n2}&\\ldots&p_{nn} \n",
        "                \\end{pmatrix},\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "where $\\displaystyle 0\\leq p_{ij} \\leq 1$ $(i,j=1,2,\\ldots, n)$ and $\\sum_{j=1}^np_{ij}=1$.\n",
        "\n",
        "It is clear that the connection between the state vector \n",
        "$\\displaystyle\\mathbf{x}_k=[x^{(1)}_k,x^{(2)}_k,\\ldots,x^{(n)}_k]^T$ at one particular stage\n",
        "and $\\displaystyle \\mathbf{x}_{k+1}=[x^{(1)}_{k+1},x^{(2)}_{k+1},\\ldots,x^{(n)}_{k+1}]^T$ at \n",
        "the next is given by\n",
        "\n",
        "$$\n",
        " \\mathbf{x}_{k+1} = P^T\\mathbf{x}_k.\n",
        "$$\n",
        "\n",
        "Then, given an initial state vector $\\displaystyle\\mathbf{x}_0$, we know that \n",
        "$\\displaystyle\\mathbf{x}_k=\\left(P^T\\right)^k\\mathbf{x}_0$.\n",
        "\n",
        "Very often we do not construct $\\displaystyle\\mathbf{x}_k$ from the numbers in each of the \n",
        "states but from the proportions of the total number of elements in the process\n",
        "which are in each of these states. Then, clearly $\\displaystyle 0\\leq x^{(i)}_k\\leq 1$ and \n",
        "$\\displaystyle\\sum_{i=1}^nx^{(i)}_k=1$. In this case, we can call $\\displaystyle\\mathbf{x}_k$ a \n",
        "probability vector.\n",
        "\n",
        "As we have seen before, the long term behaviour of a Markov process depends on \n",
        "the eigenvalues of the matrix $P^T$ and it is easy to show that one of these \n",
        "eigenvalues is $1$. For, consider\n",
        "\n",
        "$$\n",
        " |P^T-I_n|&=\\left|\\begin{pmatrix}\n",
        "                   p_{11}-1&p_{21}&\\ldots&p_{n1}\\\\\n",
        "                   p_{12}&p_{22}-1&\\ldots&p_{n2}\\\\\n",
        "                   \\vdots&\\vdots&&\\vdots\\\\\n",
        "                   p_{1n}&p_{2n}&\\ldots&p_{nn}-1\n",
        "                  \\end{pmatrix}\n",
        "\\right|,\\\\\n",
        " &=\\left|\\begin{pmatrix}\n",
        "                   \\sum_{i=1}^np_{1i}-1&\\sum_{i=1}^np_{2i}-1&\n",
        "                   \\ldots&\\sum_{i=1}^np_{ni}-1\\\\\n",
        "                   p_{12}&p_{22}-1&\\ldots&p_{n2}\\\\\n",
        "                   \\vdots&\\vdots&&\\vdots\\\\\n",
        "                   p_{1n}&p_{2n}&\\ldots&p_{nn}-1\n",
        "                  \\end{pmatrix}\n",
        "\\right|,\\\\  &=\\left|\\begin{pmatrix}\n",
        "                   0&0&\\ldots&0\\\\\n",
        "                   p_{12}&p_{22}-1&\\ldots&p_{n2}\\\\\n",
        "                   \\vdots&\\vdots&&\\vdots\\\\\n",
        "                   p_{1n}&p_{2n}&\\ldots&p_{nn}-1\n",
        "                  \\end{pmatrix}\n",
        "\\right|=0.\n",
        "$$\n",
        "\n",
        "We have thus shown that there is a potential stationary vector for this process,\n",
        "this being the eigenvector (of $\\displaystyle P^T$ or equivalently the left-eigenvector of\n",
        "$\\displaystyle P$) which corresponds to the eigenvalue $1$.\n",
        "\n",
        "There are then two questions which we can ask about this eigenvector and the\n",
        "first is whether we can be sure that its elements are all non-negative, so \n",
        "that it can conform to the requirement of being a probability vector. The\n",
        "second question concerns whether the Markov process under consideration will \n",
        "always settle down to this vector eventually, whatever the starting point \n",
        "$\\displaystyle\\mathbf{x}_0$.\n",
        "\n",
        "We can answer both of these questions in the affirmative if it can be shown that\n",
        "the eigenvalue $1$ is strictly dominant, as was the case for the eigenvalue\n",
        "$\\displaystyle\\lambda_1$ of a Leslie matrix under certain conditions.\n",
        "\n",
        "Now, the dominance of the eigenvalue $1$ can be established by means of a result\n",
        "that states that the magnitude of any eigenvalue of a given matrix does not\n",
        "exceed $\\displaystyle\\min(\\rho,\\gamma)$, where $\\rho$ is the maximum sum of the magnitudes of\n",
        "the elements in any one of its rows and $\\gamma$ is the corresponding maximum\n",
        "for its columns.\n",
        "\n",
        "Clearly, for a transition matrix $\\displaystyle\\min(\\rho,\\gamma)\\leq 1$, so the eigenvalue\n",
        "$1$ is dominant. However, as we can see from the following example, it is not \n",
        "necessarily strictly dominant.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Example 3.2.5** The matrix \n",
        "\n",
        "$$\n",
        "P_1=\\begin{pmatrix}0&1\\\\1&0\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "has eigenvalues $\\pm{1}$. Also, the probability eigenvector corresponding \n",
        "to the eigenvalue $1$ is $[\\frac{1}{2}, \\frac{1}{2}]^T$ and it is easy to \n",
        "see that there are starting points which do not lead to this vector. For example, if\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_0 = \\begin{pmatrix}\n",
        "                                        0\\\\1\n",
        "                                       \\end{pmatrix},\n",
        "\\mathbf{x}_1=\\begin{pmatrix}\n",
        " 1\\\\0\n",
        "\\end{pmatrix}=\\mathbf{x}_3=\\mathbf{x}_5,\\ldots, \\quad\n",
        "\\mathbf{x}_2=\\begin{pmatrix}\n",
        "              1\\\\0\n",
        "             \\end{pmatrix}=\\mathbf{x}_4=\\mathbf{x}_6,\\ldots.        \n",
        "$$\n",
        "\n",
        "There is a condition which establishes the strict dominance of the eigenvalue,\n",
        "and this is that the transition matrix $P$ should be regular, in that there is\n",
        "some power $P^r$ which has elements which are all positive. When this occurs \n",
        "we know that the Markov process under consideration will always settle down\n",
        "to the appropriate stationary vector.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.6** The matrix $P_1$ above is not regular because it is easy \n",
        "to see that \n",
        "\n",
        "$$\n",
        " P^{2r}=I_2\\qquad\\&\\qquad P^{2r+1}=P_1\\qquad (r=0,1,2,\\ldots).\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 3.2.7** The matrix\n",
        "\n",
        "$$\n",
        "P_2=\\begin{pmatrix}0.2&0.8\\\\0.6&0.4\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "is clearly regular and has eigenvalues given by\n",
        "\n",
        "$$\n",
        " |P-\\lambda I_2| = (\\lambda-1)(\\lambda+0.4)=0 \\qquad \n",
        " \\text{i.e., $\\lambda=1$ or $\\lambda=-0.4$.}\n",
        "$$\n",
        "\n",
        "The left eigenvector corresponding to\n",
        "$\\lambda=1$ is given by \n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}-0.8&0.6\\\\0.8&-0.6\\end{pmatrix}\\begin{pmatrix} u_1\\\\u_2\\end{pmatrix}=\\mathbf{0},\n",
        "$$\n",
        "\n",
        "and is therefore of the form $\\displaystyle [3a, 4a]^T$.\n",
        "\n",
        "As a probability vector, we have \n",
        "\n",
        "$$\n",
        "\\mathbf{u}=\\begin{pmatrix}\\frac{3}{7}\\\\\\frac{4}{7}\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "For example, suppose \n",
        "\n",
        "$$\n",
        "\\mathbf{x}_0=\\begin{pmatrix}0.5\\\\0.5\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Then\n",
        "\n",
        "$$\n",
        " \\mathbf{x}_1=\\begin{pmatrix} 0.4\\\\0.6\\end{pmatrix}, \\mathbf{x}_2=\\begin{pmatrix} 0.44\\\\0.56\\end{pmatrix}, \n",
        " \\mathbf{x}_3=\\begin{pmatrix} 0.424\\\\0.576\\end{pmatrix} \\text{ and } \n",
        " \\mathbf{x}_4=\\begin{pmatrix} 0.4304\\\\0.5696\\end{pmatrix} \\qquad \n",
        " \\left(\\text{c.f.} \\begin{pmatrix}\\frac{3}{7}\\\\\\frac{4}{7}\\end{pmatrix}=\n",
        " \\begin{pmatrix} 0.4286\\\\0.5714\\end{pmatrix}\\right).\n",
        "$$"
      ]
    }
  ]
}
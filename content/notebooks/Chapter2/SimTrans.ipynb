{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimTrans.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAEAnUdln2/NC1AwcSNnwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjcrofts77/Linear-Systems-MATH30451/blob/main/content/notebooks/Chapter2/SimTrans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkoqkKZl5oih"
      },
      "source": [
        "# 2.1 Similarity Transformations and the Jordan Canonical Form\n",
        "\n",
        "Recall that if $A$ is a real symmetric matrix, and if we choose $P$ to be the matrix whose columns are the orthonormal eigenvectors of $A$, then the matrix $D=P^{-1}AP$ is diagonal, with non-zero elements given by the eigenvalues of $A$. However, if the matrix $A$ is not symmetric then diagonalisation might not be possible. For such systems, a structure, which is close to diagonal form, known as the Jordan canonical form, can be obtained.\n",
        "\n",
        "An $n\\times n$ matrix is diagonalisable if and only if the sum of the dimensions of the eigenspaces is $n$. Or, equivalently, if and only if $A$ has $n$ linearly independent eigenvectors. As we saw in DEM (DETM) not all matrices are diagonalisable.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Background Material\n",
        "\n",
        "Before moving on let us review some key ideas from linear algebra.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let $f$ be a linear map from a vector space $V$ to itself, and $A$ the matrix representing $f$ w.r.t the standard basis.\n",
        "\n",
        "````{margin}\n",
        "```{important}\n",
        "Hopefully most of this is familiar but if not don't worry! We'll cover it in the lectures in more detail.\n",
        "```\n",
        "````\n",
        "\n",
        "1. Then the *characteristic polynomial* is $\\displaystyle\\chi_A(t) =\n",
        "\\det{(A-tI)}$.\n",
        "\n",
        "2. $\\lambda$ is an *eigenvalue* of $A$ if $\\displaystyle\\chi_A(\\lambda)=0$.\n",
        "\n",
        "3. $v$ is an *eigenvector* for the eigenvalue $\\lambda$ if\n",
        "$\\displaystyle Av=\\lambda v$.\n",
        "\n",
        "4. The *eigenspace* for the eigenvalue $\\lambda$ is\n",
        "$\\displaystyle\\ker{(A-\\lambda I)}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 2.1.1**  The matrices\n",
        "\n",
        "$$\n",
        " A_1 = \\begin{pmatrix} 1~ &0\\\\0~ & 1\\end{pmatrix} \\quad\\text{and}\\quad A_2 = \\begin{pmatrix} 1~ &1\\\\0~ & 1\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "have the same characteristic polynomial given by\n",
        "\n",
        "$$\n",
        "\\chi_{A_1}(t) = \\chi_{A_2}(t) = (t-1)^2\n",
        "$$\n",
        "\n",
        "which implies that $\\displaystyle \\lambda=1$ has AM=2 in both cases.\n",
        "\n",
        "The corresponding eigenspaces are given by\n",
        "\n",
        "\n",
        "- $A_1:$\n",
        " \n",
        " $$\n",
        " (I-A_1)\\mathbf{v} = \\begin{pmatrix} 0~ & 0\\\\0~&0\\end{pmatrix}\\mathbf{v}=\\mathbf{0}\n",
        " $$\n",
        "\n",
        " which $\\implies$ $v_1 = \\alpha$ and $v_2 = \\beta$. Therefore\n",
        "\n",
        " $$\n",
        "  \\text{Eigenspace - }\\quad \\left\\{ \\alpha\\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\beta\\begin{pmatrix} 0\\\\1\\end{pmatrix} : \\alpha, \\beta\\in\\mathbb{R} \\right\\}\n",
        " $$\n",
        "\n",
        " and the GM=2. ($\\therefore A_1$ is diagonalisable (trivially).)\n",
        "\n",
        "- $A_2$:\n",
        "\n",
        " $$\n",
        " (I-A_2)\\mathbf{v} = \\begin{pmatrix} 0~ & -1\\\\0~&0\\end{pmatrix}\\mathbf{v}=\\mathbf{0}\n",
        " $$\n",
        "\n",
        " which $\\implies$ $v_1 = \\alpha$ and $v_2 =0$. Therefore\n",
        "\n",
        " $$\n",
        "  \\text{Eigenspace - }\\quad \\left\\{ \\alpha\\begin{pmatrix} 1\\\\0\\end{pmatrix} : \\alpha\\in\\mathbb{R} \\right\\}\n",
        " $$\n",
        "\n",
        " and the GM=1. (NOT diagonalisable.)\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Theorem 2.1.1 (Cayley-Hamilton)** A matrix satisfies its own characteristic equation, i.e., $\\displaystyle\\chi_A(A)=0$.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 2.1.2** Let $\\displaystyle A = \\begin{pmatrix} 1~&1\\\\2~&1\\end{pmatrix}$ then\n",
        "\n",
        "$$\n",
        "\\chi_A(t) &= \\left| \\begin{array}{cc}t-1&1\\\\2&t-1\\end{array}\\right|\\\\\n",
        "              &= (t-1)^2-2\\\\\n",
        "              &= t^2-2t-1\n",
        "$$\n",
        "\n",
        "Now to check C-H we compute\n",
        "\n",
        "$$\n",
        " \\chi_A(A) &= \\begin{pmatrix} 1~&1\\\\2~&1\\end{pmatrix}^2-2\\begin{pmatrix}1~&1\\\\2~&1\\end{pmatrix}-\\begin{pmatrix}1~&0\\\\0~&1\\end{pmatrix}\\\\\n",
        "                 &= \\begin{pmatrix}3~&2\\\\4~&3\\end{pmatrix}-\\begin{pmatrix}2~&2\\\\4~&2\\end{pmatrix}-\\begin{pmatrix}1~&0\\\\0~&1\\end{pmatrix}\\\\\n",
        "                 &= \\begin{pmatrix}0~&0\\\\0~&0\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "as expected.\n",
        "\n",
        "It is sometimes useful to use C-H to compute matrix powers.\n",
        "\n",
        "<br>\n",
        " \n",
        "**Example 2.1.3** For $\\displaystyle A = \\begin{pmatrix}1&2\\\\3&4\\end{pmatrix}$ we have that\n",
        "\n",
        "$$\n",
        "\\chi_A(t) = \\left|\\begin{array}{cc}1-t&-2\\\\-3&4-t\\end{array}\\right| = t^2-5t-2\n",
        "$$\n",
        "\n",
        "so that\n",
        "\n",
        "$$\n",
        "A^2-5A-2I_2 = 0\\implies A^2 = 5A+2I_2.\n",
        "$$ (CHpower)\n",
        "\n",
        "````{margin}\n",
        "```{note}\n",
        "We denote by $I_n$ the $n\\times n$ identity matrix\n",
        "```\n",
        "````\n",
        "Now using {eq}`CHpower` we can work out any power, $\\displaystyle A^k$, in terms of just $A$ and $I_2$.\n",
        "\n",
        "So, for example, \n",
        "\n",
        "$$\n",
        " A^3 &= (5A+2I_2)A = 5A^2+2A = 5(5A+2I_2)+2A = 27A+10I_2\\\\\n",
        " A^4 &= A^3A = (27A+10I_2)A = 27A^2+10A = \\cdots = 145A+54I_2.\n",
        "$$ \n",
        "\n",
        "*etc.*\n",
        "\n",
        "We can also work out the inverse of $A$:\n",
        "\n",
        "$$\n",
        "A^2=5A+2I_2\\quad\\implies\\quad A^{-1} = \\frac{A-5I_2}{2}.\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "We shall make good use of the Caley-Hamilton theorem throughout this course!\n",
        "\n",
        "<br>\n",
        "\n",
        "Another concept that will prove useful is that of the so-called minimal polynomial which we define below.\n",
        "\n",
        "<br>\n",
        "\n",
        "````{margin} Monic polynomials\n",
        "- $\\displaystyle 3t^2+t-1$ is *not* monic\n",
        "- $\\displaystyle t^2+t-2$ *is* monic\n",
        "````\n",
        "\n",
        "> **Definition 2.1.2** The *minimal polynomial*, $\\displaystyle m_A(t)$, of $\\displaystyle A$ is the monic polynomial of least degree for which $\\displaystyle m_A(A)=0$.\n",
        "\n",
        "<br>\n",
        "\n",
        "Note that *monic* means that the coefficient of the term with highest power in $\\displaystyle m_A(t)$ equals one.\n",
        "\n",
        "```{note}\n",
        "- The minimal polynomial of $A$ is unique, and divides the characteristic polynomial.\n",
        "- This means that all the factors of $\\displaystyle m_A(t)$ are also factors of $\\displaystyle \\chi_A(t)$.\n",
        "- We can use this to determine $\\displaystyle m_A(t)$.\n",
        "- If $P$ is any invertible matrix, then $\\displaystyle m_A(P^{-1}AP)=0$.\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Theorem 2.1.3** The minimal polynomial has the form \n",
        ">\n",
        ">$$m_A(t) = (t-\\lambda_1)^{e_1}\\cdots(t-\\lambda_{n_k})^{e_k}\n",
        "$$\n",
        ">\n",
        "> where $\\lambda_1,\\ldots, \\lambda_{n_k}$ are the eigenvalues of the matrix $A$, and the numbers $e_i$ are such that $1\\leq e_i\\leq r_i$ with $r_i$ the algebraic multiplicity of $\\lambda_i$. Moreover, $A$ is diagonalisable if and only if each $e_i=1$.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 2.1.4** Consider $\\displaystyle \\chi_{A_2}(t) = (t-1)^2$ then from the above we know that $\\displaystyle m_{A_2}(t)$ is either\n",
        "\n",
        "$$\n",
        " \\text{(i) } (t-1)^2 \\quad\\text{or} \\quad \\text{(ii) } t-1.\n",
        "$$\n",
        "\n",
        "Let's check\n",
        "\n",
        "$$\n",
        " A - I_2 = \\begin{pmatrix} 1~& 1\\\\0~& 1\\end{pmatrix} - \\begin{pmatrix} 1~& 0\\\\0~& 1\\end{pmatrix} = \\begin{pmatrix} 0~& 1\\\\0~& 0\\end{pmatrix}\\neq 0\n",
        "$$\n",
        "\n",
        "Therefore $\\displaystyle m_{A_2}(t) = (t-1)^2$.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example 2.1.5** Now suppose we have $\\displaystyle A_3 = \\begin{pmatrix} 1~&1~&0\\\\0~&1~&0\\\\0~&0~&1\\end{pmatrix}.$\n",
        "\n",
        "\n",
        "Then \n",
        "\n",
        "$$ \\chi_{A_3}(t) = \\left|\\begin{array}{ccc}t-1&-1&0\\\\0&t-1&0\\\\0&0&t-1\\end{array}\\right| = (t-1)^3$$ \n",
        "\n",
        "and so $m_{A_3}(t)$ can be one of \n",
        "\n",
        "$$\n",
        " \\text{(i) } (t-1)^3, \\quad \\text{(ii) } (t-1)^2 \\quad \\text{or} \\quad \\text{(iii) }  t-1.\n",
        "$$\n",
        "\n",
        "First we check\n",
        "\n",
        "$$\n",
        " (A-I_3)^2 = \\begin{pmatrix} 0~&1~&0\\\\0~&0~&0\\\\0~&0~&0 \\end{pmatrix}^2\n",
        "                 = \\begin{pmatrix} 0~&0~&0\\\\0~&0~&0\\\\0~&0~&0 \\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Therefore $\\displaystyle m_{A_3}(t) = (t-1)^2 = t^2-2t+1$.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Theorem 2.1.4 (Primary Decomposition Theorem)** Let $\\displaystyle f:V\\mapsto V$ be linear and represented by the matrix $A$ with minimal polynomial $\\displaystyle m_A(t) = (t-\\lambda_1)^{e_1}\\ldots(t-\\lambda_r)^{e_r}$ where $\\displaystyle \\lambda_1,\\ldots, \\lambda_r$ are distinct eigenvalues of $A$ and $\\displaystyle e_1,\\ldots,e_r$ are natural numbers. Let $\\displaystyle V_1,\\ldots, V_r$ denote the generalised eigenspace ($\\displaystyle V_i=\\ker\\left[(A-\\lambda_i)^{e_i}\\right]$). Then $\\displaystyle V = V_1\\bigoplus V_2 \\cdots\\bigoplus V_r$.\n",
        "\n",
        "<br>\n",
        "\n",
        "```{note}\n",
        "The above theorem says that two (or more) eigenvectors with distinct eigenvalues are linearly independent (among other things).\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "Importantly, if the factor $(t-\\lambda_i)$ appears in the minimal polynomial with multiplicity $e_i$ (i.e., $\\displaystyle m_A(t)=(t-\\lambda_i)^{e_i}g(t)$ for some polynomial $g(t)$ which does not have $(t-\\lambda_i)$ as a factor), then the *generalised eigenspace* is $\\displaystyle\\ker\\left[(A-\\lambda_i I)^{e_i}\\right]$.\n",
        "\n",
        "<br>\n",
        "\n",
        ">**Proposition 2.1.5** Using the notation from Theorem 2.1.2\n",
        ">\n",
        ">$$\n",
        "\\ker(A-\\lambda_iI)\\subset\\ker\\left[(A-\\lambda_iI)^2\\right]\\subset\\cdots \\subset\\ker\\left[(A-\\lambda_iI)^{e_i}\\right].\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The results above will hopefully help to motivate some of the arguments that follow in the next few sections.\n",
        "\n"
      ]
    }
  ]
}
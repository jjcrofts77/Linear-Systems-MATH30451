{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimTrans.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9SLbM+aVgIyX/UYQXior8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjcrofts77/Linear-Systems-MATH30451/blob/main/content/notebooks/Chapter2/SimTrans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkoqkKZl5oih"
      },
      "source": [
        "# 2.1 Similarity Transformations and the Jordan Canonical Form\n",
        "\n",
        "Recall that if $A$ is a real symmetric matrix, and if we choose $P$ to be the matrix whose columns are the orthonormal eigenvectors of $A$, then the matrix $D=P^{-1}AP$ is diagonal, with non-zero elements given by the eigenvalues of $A$. However, if the matrix $A$ is not symmetric then diagonalisation might not be possible. For such systems, a structure, which is close to diagonal form, known as the Jordan canonical form, can be obtained.\n",
        "\n",
        "An $n\\times n$ matrix is diagonalisable if and only if the sum of the dimensions of the eigenspaces is $n$. Or, equivalently, if and only if $A$ has $n$ linearly independent eigenvectors. As we saw in DETM not all matrices are diagonalisable.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Background Material\n",
        "\n",
        "Before moving on let us review some key ideas from linear algebra.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let $f$ be a linear map from a vector space $V$ to itself, and $A$ the matrix representing $f$ w.r.t the standard basis.\n",
        "\n",
        "````{margin}\n",
        "```{important}\n",
        "Hopefully most of this is familiar but if not don't worry! We'll cover it in the lectures in more detail.\n",
        "```\n",
        "````\n",
        "\n",
        "1. Then the *characteristic polynomial* is $\\displaystyle\\chi_A(t) =\n",
        "\\det{(A-tI)}$.\n",
        "\n",
        "2. The *minimal polynomial* is the monomial (which means a\n",
        "polynomial with the coefficient of the highest power being equal to one) of least degree such that $m_A(A)=0$. The minimal polynomial is unique and divides the characteristic polynomial. Also, if $P$ is any invertible matrix, then $\\displaystyle m_A(P^{-1}AP)=0$.\n",
        "\n",
        "3. $\\lambda$ is an *eigenvalue* of $A$ if $\\displaystyle\\chi_A(\\lambda)=0$.\n",
        "\n",
        "4. $v$ is an *eigenvector* for the eigenvalue $\\lambda$ if\n",
        "$\\displaystyle Av=\\lambda v$.\n",
        "\n",
        "5. The *eigenspace* for the eigenvalue $\\lambda$ is\n",
        "$\\displaystyle\\ker{(A-\\lambda I)}$.\n",
        "\n",
        "6. If the factor $(t-\\lambda)$ appears in the minimal polynomial with multiplicity $k$ (i.e., $\\displaystyle m_A(t)=(t-\\lambda)^kg(t)$ for some polynomial $g(t)$ which does not have $(t-\\lambda)$ as a factor), then the *generalised eigenspace* is $\\displaystyle\\ker\\left[(A-\\lambda I)^k\\right]$.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Theorem 2.1.2 (Primary Decomposition Theorem)** Let $\\displaystyle f:V\\mapsto V$ be linear and represented by the matrix $A$ with minimal polynomial $\\displaystyle m_A(t) = (t-\\lambda_1)^{e_1}\\ldots(t-\\lambda_r)^{e_r}$ where $\\displaystyle \\lambda_1,\\ldots, \\lambda_r$ are distinct eigenvalues of $A$ and $\\displaystyle e_1,\\ldots,e_r$ are natural numbers. Let $\\displaystyle V_1,\\ldots, V_r$ denote the generalised eigenspace ($\\displaystyle V_i=\\ker\\left[(A-\\lambda_i)^{e_i}\\right]$). Then $\\displaystyle V = V_1\\bigoplus V_2 \\cdots\\bigoplus V_r$.\n",
        "\n",
        "<br>\n",
        "\n",
        "```{note}\n",
        "The above theorem says that two (or more) eigenvectors with distinct eigenvalues are linearly independent (among other things).\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Theorem 2.1.2 (Cayley-Hamilton)** A matrix satisfies its own characteristic equation, i.e., $\\displaystyle\\chi_A(A)=0$.\n",
        "\n",
        "<br>\n",
        "\n",
        "We shall make good use of the Caley-Hamilton theorem throughout this course!\n",
        "\n",
        ">**Proposition 2.1.3** Using the notation from Theorem 2.1\n",
        ">\n",
        ">$$\n",
        "\\ker(A-\\lambda_iI)\\subset\\ker\\left[(A-\\lambda_iI)^2\\right]\\subset\\cdots \\subset\\ker\\left[(A-\\lambda_iI)^{e_i}\\right].\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The three results above will hopefully help to motivate some of the arguments that follow in the next few sections.\n",
        "\n"
      ]
    }
  ]
}